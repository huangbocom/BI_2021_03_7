### Thinking1：机器学习中的监督学习、非监督学习、强化学习有何区别
** 监督学习是基于lable做loss function
** 非监督学习是无lable,只得到特征
** 强化学习是无label,只有不实时的奖励信号,与监督学习的区别，没有监督学习已经准备好的训练数据输出值，强化学习只有奖励值，
### Thinking2：什么是策略网络，价值网络，有何区别
** 价值网络：通过计算目前状态s的累积分数的期望，价值网络给游戏中的状态赋予一个分数（数值），每个状态都经历了整个数值网络
** 策略网络：对于给定的输入，通过学习给出一个确定输出的网络：（动作1， 状态1）。。。 （动作n， 状态n）
** 区别：
策略网络是一个落子的概率分布，如蒙特卡洛
价值网络是一个可能获胜的数值。
策略能学到一些随机策略，价值学不到随时策略。

### Thinking3：请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的
** MCTS的原理：蒙特卡洛树搜索，结合了随机模拟的一般性和树搜索的准确性
MCTS是一个搜索算法，它采用的各种方法都是为了有效地减少搜索空间。在MCTS的每一个回合，起始内容是一个半展开的搜索树，目标是原先的半展开+再多展开一个/一层节点的搜索树
MCTS的作用是通过模拟来进行预测输出结果，理论上可以用于以{state,action}为定义的任何领域

** 有4步：选择，扩展，模拟，回传
选择，从root开始，按policy，去搜索叶子结点
扩展：对leaf扩展成一个或多个合法的子节点。
模拟：对子节点采用随机的方式模拟若干次实验，得到当前模拟的所得分数
回传：根据子节点的得分，更新当时子节点的模拟次数与得分值，同时更新祖先节点。
### Thinking4：假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑
** 用户在抖音上看的形为可以看成是实现与推荐系统的多轮交互，
此过程中推荐系统能够感知用户的实时行为，从而更加理解用户，在接下来的交互中提供更好的体验。
在这样的多轮交互中，我们把推荐系统看作智能体（Agent），用户看作环境（Environment），推荐系统与用户的多轮交互过程可以建模：

State：Agent对Environment的观测，即用户的意图和所处场景。
Action：以mtcs对推荐列表做调整，考虑长期收益对当前决策的影响。
Reward：根据用户反馈给予Agent相应的奖励，为业务目标直接负责。
P(s,a)：Agent在当前State s下采取Action a的状态转移概率。
### Thinking5：在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路
** 我觉得强化学习应用在无人驾驶领域需要解决的一个最大的问题就是训练空间无穷大的问题。
对于一辆车来说，他的可行使空间几乎是无穷大的，
他可能遇到的场景太多了，你不可能建立一个全世界的仿真用于一辆车的训练，
但是从理论上来讲，它就应该在全世界的仿真环境中做训练才行。
这和围棋就不一样了，围棋再怎么样，棋盘就那么大，是有限的，
情景一定是可穷举的。而且reward也非常清晰。
但是强化学习应用到无人驾驶的时候，就要面临训练环境的问题。
开车不是下棋，关乎性命。灰盒模型没人敢用。
所以我觉得未来应该是 传统方法+强化学习 才更有希望付诸实践。



